{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T23:42:12.476248Z",
     "start_time": "2018-06-23T23:42:06.278082Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the training data...\n",
      "\n",
      "\n",
      "Training data loaded!\n",
      "Creating Data Augmenter...\n",
      "Finished making data augmenter...\n",
      "Creating the model...\n",
      "Model created!\n",
      "Training model...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "All input arrays (x) should have the same number of samples.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-12-4e9a080cbb53>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    295\u001b[0m                               \u001b[0mnb_val_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX_num_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m                               \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m                               callbacks=[best_model])\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Loading the best model...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liheyuan/ENV/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mfit_generator\u001b[0;34m(self, generator, samples_per_epoch, nb_epoch, verbose, callbacks, validation_data, nb_val_samples, class_weight, max_q_size, nb_worker, pickle_safe, initial_epoch)\u001b[0m\n\u001b[1;32m   1501\u001b[0m                                  str(validation_data))\n\u001b[1;32m   1502\u001b[0m             val_x, val_y, val_sample_weights = self._standardize_user_data(\n\u001b[0;32m-> 1503\u001b[0;31m                 val_x, val_y, val_sample_weight)\n\u001b[0m\u001b[1;32m   1504\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalidation_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_sample_weights\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1505\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liheyuan/ENV/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1039\u001b[0m                           \u001b[0;32mfor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mref\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1040\u001b[0m                           in zip(y, sample_weights, class_weights, self.sample_weight_modes)]\n\u001b[0;32m-> 1041\u001b[0;31m         \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1042\u001b[0m         \u001b[0mcheck_loss_and_target_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_output_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/liheyuan/ENV/local/lib/python2.7/site-packages/keras/engine/training.pyc\u001b[0m in \u001b[0;36mcheck_array_lengths\u001b[0;34m(inputs, targets, weights)\u001b[0m\n\u001b[1;32m    177\u001b[0m     \u001b[0mset_x\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mset_x\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 179\u001b[0;31m         raise ValueError('All input arrays (x) should have '\n\u001b[0m\u001b[1;32m    180\u001b[0m                          'the same number of samples.')\n\u001b[1;32m    181\u001b[0m     \u001b[0mset_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_lengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: All input arrays (x) should have the same number of samples."
     ]
    }
   ],
   "source": [
    "# -*- coding: UTF-8 -*-\n",
    "\n",
    "# 参数\n",
    "model_idx = 10\n",
    "length_per_contour=200\n",
    "contours_data = '../middata/contours_data.npy'\n",
    "root = '../rawdata'\n",
    "\n",
    "\n",
    "split_random_state = 7\n",
    "split = .9\n",
    "\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.preprocessing.image import img_to_array, load_img\n",
    "from keras.preprocessing.image import ImageDataGenerator, NumpyArrayIterator, array_to_img\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Dropout, Activation, Convolution2D, MaxPooling2D, Flatten, Input, merge, Reshape, Convolution1D, MaxPooling1D\n",
    "\n",
    "from generate_contour_data_and_augment import load_contours\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def load_numeric_training(standardize=True):\n",
    "    data = pd.read_csv(os.path.join(root, 'train.csv'))\n",
    "    ID = data.pop('id')\n",
    "    y = data.pop('species')\n",
    "    y = LabelEncoder().fit(y).transform(y)\n",
    "    X = StandardScaler().fit(data).transform(data) if standardize else data.values\n",
    "\n",
    "    return ID, X, y\n",
    "\n",
    "\n",
    "def load_numeric_test(standardize=True):\n",
    "    test = pd.read_csv(os.path.join(root, 'test.csv'))\n",
    "    ID = test.pop('id')\n",
    "    test = StandardScaler().fit(test).transform(test) if standardize else test.values\n",
    "    return ID, test\n",
    "\n",
    "\n",
    "def resize_img(img, max_dim=96):\n",
    "    \"\"\"\n",
    "    如果图片放歪了或者放倒了，将其扶正\n",
    "    \"\"\"\n",
    "    # Get the axis with the larger dimension\n",
    "    max_ax = max((0, 1), key=lambda i: img.size[i])\n",
    "    # Scale both axes so the image's largest dimension is max_dim\n",
    "    scale = max_dim / float(img.size[max_ax])\n",
    "    return img.resize((int(img.size[0] * scale), int(img.size[1] * scale)))\n",
    "\n",
    "\n",
    "def load_image_data(ids, max_dim=96, center=True):\n",
    "    \"\"\"\n",
    "    将所有的图片统一成96x96大小\n",
    "    \"\"\"\n",
    "    X = np.empty((len(ids), max_dim, max_dim, 1))\n",
    "    for i, idee in enumerate(ids):\n",
    "        x = resize_img(load_img(os.path.join(root, 'images', str(idee) + '.jpg'), grayscale=True), max_dim=max_dim)\n",
    "        x = img_to_array(x)\n",
    "        length = x.shape[0]\n",
    "        width = x.shape[1]\n",
    "        if center:\n",
    "            h1 = int((max_dim - length) / 2)\n",
    "            h2 = h1 + length\n",
    "            w1 = int((max_dim - width) / 2)\n",
    "            w2 = w1 + width\n",
    "        else:\n",
    "            h1, w1 = 0, 0\n",
    "            h2, w2 = (length, width)\n",
    "        X[i, h1:h2, w1:w2, 0:1] = x\n",
    "    return np.around(X / 255.0)\n",
    "\n",
    "# def load_contours(ID):\n",
    "#     \"\"\"目前这是个假的加载函数，仅仅为了调试加上边缘数据后的代码能否跑通\"\"\"\n",
    "#     length = len(ID)\n",
    "#     return np.array([[0] * 100] * length)\n",
    "\n",
    "def fool_load_shape_and_blade_data_list(id_list):\n",
    "    length = len(id_list)\n",
    "    print ''\n",
    "    tmp = np.array([[[0] * 511] * 2] * length)\n",
    "#     return tmp[:,0,:], tmp[:,1,:]\n",
    "    return tmp.reshape((length,511,2))\n",
    "\n",
    "\n",
    "def load_train_data(split=split, random_state=None):\n",
    "    ID, X_num_tr, y = load_numeric_training()\n",
    "    X_img_tr = load_image_data(ID)\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, train_size=split, random_state=random_state)\n",
    "    train_ind, test_ind = next(sss.split(X_num_tr, y, ID))\n",
    "    X_num_val, X_img_val, y_val, ID_val = X_num_tr[test_ind], X_img_tr[test_ind], y[test_ind], ID[test_ind]\n",
    "    X_num_tr, X_img_tr, y_tr, ID_tr = X_num_tr[train_ind], X_img_tr[train_ind], y[train_ind], ID[train_ind]\n",
    "    \n",
    "    contours_tr = load_contours(ID_tr, contours_data)\n",
    "    shape_contours_tr = fool_load_shape_and_blade_data_list(ID_tr)\n",
    "    contours_val = load_contours(ID_val, contours_data)\n",
    "    shape_contours_val = fool_load_shape_and_blade_data_list(ID_val)\n",
    "    return (X_num_tr, X_img_tr, y_tr, contours_tr, shape_contours_tr), (X_num_val, X_img_val, y_val, contours_val, shape_contours_val)\n",
    "\n",
    "\n",
    "def load_test_data():\n",
    "    ID, X_num_te = load_numeric_test()\n",
    "    X_img_te = load_image_data(ID)\n",
    "    contours_test = load_contours(ID, contours_data)\n",
    "    shape_contours_test = fool_load_shape_and_blade_data_list(ID)\n",
    "    return ID, X_num_te, X_img_te, contours_test, shape_contours_test\n",
    "\n",
    "# 加载数据\n",
    "print('Loading the training data...')\n",
    "(X_num_tr, X_img_tr, y_tr, contours_tr, shape_contours_tr), (X_num_val, X_img_val, y_val, contours_val, shape_contours_val) = \\\n",
    "                                    load_train_data(random_state=split_random_state)\n",
    "y_tr_cat = to_categorical(y_tr)\n",
    "y_val_cat = to_categorical(y_val)\n",
    "print('Training data loaded!')\n",
    "\n",
    "\n",
    "class ImageDataGenerator2(ImageDataGenerator):\n",
    "    \"\"\"图像数据生成器\"\"\"\n",
    "    def flow(self, X, y=None, batch_size=32, shuffle=True, seed=None,\n",
    "             save_to_dir=None, save_prefix='', save_format='jpeg'):\n",
    "        return NumpyArrayIterator2(\n",
    "            X, y, self,\n",
    "            batch_size=batch_size, shuffle=shuffle, seed=seed,\n",
    "            dim_ordering=self.dim_ordering,\n",
    "            save_to_dir=save_to_dir, save_prefix=save_prefix, save_format=save_format)\n",
    "\n",
    "\n",
    "class NumpyArrayIterator2(NumpyArrayIterator):\n",
    "    \"\"\"预提取数据生成器\"\"\"\n",
    "    def next(self):\n",
    "        with self.lock:\n",
    "            self.index_array, current_index, current_batch_size = next(self.index_generator)\n",
    "        batch_x = np.zeros(tuple([current_batch_size] + list(self.x.shape)[1:]))\n",
    "\n",
    "        for i, j in enumerate(self.index_array):\n",
    "            x = self.x[j]\n",
    "            x = self.image_data_generator.random_transform(x.astype('float32'))\n",
    "            x = self.image_data_generator.standardize(x)\n",
    "            batch_x[i] = x\n",
    "        if self.save_to_dir:\n",
    "            for i in range(current_batch_size):\n",
    "                img = array_to_img(batch_x[i], self.dim_ordering, scale=True)\n",
    "                fname = '{prefix}_{index}_{hash}.{format}'.format(prefix=self.save_prefix,\n",
    "                                                                  index=current_index + i,\n",
    "                                                                  hash=np.random.randint(1e4),\n",
    "                                                                  format=self.save_format)\n",
    "                img.save(os.path.join(self.save_to_dir, fname))\n",
    "        if self.y is None:\n",
    "            return batch_x\n",
    "        batch_y = self.y[self.index_array]\n",
    "        return batch_x, batch_y\n",
    "\n",
    "print('Creating Data Augmenter...')\n",
    "imgen = ImageDataGenerator2(\n",
    "    rotation_range=10,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest')\n",
    "imgen_train = imgen.flow(X_img_tr, y_tr_cat, seed=np.random.randint(1, 10000))\n",
    "print('Finished making data augmenter...')\n",
    "\n",
    "\n",
    "def combined_model():\n",
    "\n",
    "    # 图像二维卷积模块\n",
    "    image_input = Input(shape=(96, 96, 1), name='image')\n",
    "    \n",
    "    x = Convolution2D(64, 5, 5, input_shape=(96, 96, 1), border_mode='same')(image_input)\n",
    "    x = (Activation('relu'))(x)\n",
    "    x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n",
    "\n",
    "    x = (Convolution2D(128, 5, 5, border_mode='same'))(x)\n",
    "    x = (Activation('relu'))(x)\n",
    "    x = (MaxPooling2D(pool_size=(2, 2), strides=(2, 2)))(x)\n",
    "\n",
    "    x = Flatten()(x)\n",
    "    \n",
    "    # 预选取特征MLP模块\n",
    "    numerical_input = Input(shape=(192,), name='numerical')\n",
    "    \n",
    "    numerical = Dense(128 ,activation='relu')(numerical_input)\n",
    "    \n",
    "    \n",
    "    # 预选取特征一维卷积模块\n",
    "    conv1d = (Reshape((64,3)))(numerical_input)\n",
    "    conv1d = Convolution1D(nb_filter=64, filter_length=4, border_mode='same')(conv1d)\n",
    "    conv1d = (Activation('relu'))(conv1d)\n",
    "    conv1d = (MaxPooling1D(pool_length=2, stride=2, border_mode='same'))(conv1d)\n",
    "    \n",
    "    conv1d = Convolution1D(nb_filter=128, filter_length=4, border_mode='same')(conv1d)\n",
    "    conv1d = (Activation('relu'))(conv1d)\n",
    "    conv1d = (MaxPooling1D(pool_length=2, stride=2, border_mode='same'))(conv1d)\n",
    "\n",
    "    conv1d = Flatten()(conv1d)\n",
    "    \n",
    "#     # 对轮廓线进行一维卷积\n",
    "    contour_input = Input(shape=(length_per_contour,2), name='contour')\n",
    "    \n",
    "    contour = (Reshape((length_per_contour,2)))(contour_input)\n",
    "    contour = Convolution1D(nb_filter=64, filter_length=4, border_mode='same')(contour)\n",
    "    contour = (Activation('relu'))(contour)\n",
    "    contour = (MaxPooling1D(pool_length=2, stride=2, border_mode='same'))(contour)\n",
    "    \n",
    "    contour = Convolution1D(nb_filter=128, filter_length=4, border_mode='same')(contour)\n",
    "    contour = (Activation('relu'))(contour)\n",
    "    contour = (MaxPooling1D(pool_length=2, stride=2, border_mode='same'))(contour)\n",
    "\n",
    "    contour = Flatten()(contour)\n",
    "    \n",
    "    # 对形状轮廓进行一维卷积\n",
    "    shape_contour_input = Input(shape=(511,2), name='shape_contour_input')\n",
    "    \n",
    "    shape_contour = (Reshape((511,2)))(shape_contour_input)\n",
    "    shape_contour = Convolution1D(nb_filter=64, filter_length=4, border_mode='same')(shape_contour)\n",
    "    shape_contour = (Activation('relu'))(shape_contour)\n",
    "    shape_contour = (MaxPooling1D(pool_length=2, stride=2, border_mode='same'))(shape_contour)\n",
    "    \n",
    "    shape_contour = Convolution1D(nb_filter=128, filter_length=4, border_mode='same')(shape_contour)\n",
    "    shape_contour = (Activation('relu'))(shape_contour)\n",
    "    shape_contour = (MaxPooling1D(pool_length=2, stride=2, border_mode='same'))(shape_contour)\n",
    "\n",
    "    shape_contour = Flatten()(shape_contour)\n",
    "    \n",
    "    \n",
    "    # 特征合并\n",
    "    concatenated = merge([x, numerical,conv1d, contour,shape_contour], mode='concat')\n",
    "#     concatenated = merge([x, numerical,conv1d], mode='concat')\n",
    "#     concatenated = contour\n",
    "\n",
    "\n",
    "    # dense层\n",
    "#     concatenated = Dense(1024, activation='relu')(concatenated)\n",
    "#     concatenated = Dropout(.5)(concatenated)\n",
    "\n",
    "    concatenated = Dense(512, activation='relu')(concatenated)\n",
    "    concatenated = Dropout(.5)(concatenated)\n",
    "    \n",
    "    # 输出\n",
    "    out = Dense(99, activation='softmax')(concatenated)\n",
    "\n",
    "    \n",
    "    model = Model(input=[image_input, numerical_input, contour_input, shape_contour_input], output=out)\n",
    "#     model = Model(input=[image_input, numerical_input], output=out)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "print('Creating the model...')\n",
    "model = combined_model()\n",
    "print('Model created!')\n",
    "\n",
    "\n",
    "\n",
    "def combined_generator(imgen, X, contours, shape_contours):\n",
    "    \"\"\"\n",
    "    各种数据的综合生成器\n",
    "    \"\"\"\n",
    "    while True:\n",
    "        for i in range(X.shape[0]):\n",
    "            batch_img, batch_y = next(imgen)\n",
    "            x = X[imgen.index_array]\n",
    "            contour = contours[imgen.index_array]\n",
    "            shape_contour = shape_contours[imgen.index_array]\n",
    "            print '+++++++++++++++++++'\n",
    "            print shape_contour.shape\n",
    "#             yield [batch_img, x, contour], batch_y\n",
    "            yield [batch_img, x, contour, shape_contour], batch_y\n",
    "\n",
    "\n",
    "\n",
    "            \n",
    "# autosave best Model\n",
    "best_model_file = \"../models/leafnet_\"+str(model_idx)+\".h5\"\n",
    "best_model = ModelCheckpoint(best_model_file, monitor='val_loss', verbose=1, save_best_only=True)\n",
    "\n",
    "print('Training model...')\n",
    "history = model.fit_generator(combined_generator(imgen_train, X_num_tr, contours_tr, shape_contours_tr),\n",
    "                              samples_per_epoch=X_num_tr.shape[0],\n",
    "                              nb_epoch=89,\n",
    "                              validation_data=([X_img_val, X_num_val, contours_val, shape_contours_tr], y_val_cat),\n",
    "#                               validation_data=([X_img_val, X_num_val], y_val_cat),\n",
    "                              nb_val_samples=X_num_val.shape[0],\n",
    "                              verbose=0,\n",
    "                              callbacks=[best_model])\n",
    "\n",
    "print('Loading the best model...')\n",
    "model = load_model(best_model_file)\n",
    "print('Best Model loaded!')\n",
    "\n",
    "\n",
    "# 预测测试集并生成可提交文件\n",
    "LABELS = sorted(pd.read_csv(os.path.join(root, 'train.csv')).species.unique())\n",
    "index, X_num_te, X_img_te, contours_test, shape_contours_test = load_test_data()  # index就是ID\n",
    "yPred_proba = model.predict([X_img_te,  X_num_te, contours_test, shape_contours_test])\n",
    "# yPred_proba = model.predict([X_img_te,  X_num_te])\n",
    "\n",
    "yPred = pd.DataFrame(yPred_proba,index=index,columns=LABELS)\n",
    "\n",
    "print('Creating and writing submission...')\n",
    "fp = open('../submissions/Keras_ConvNet_with_pictures_kernel_'+str(model_idx)+'.csv', 'w')\n",
    "fp.write(yPred.to_csv())\n",
    "print('Finished writing submission')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T23:57:47.979954Z",
     "start_time": "2018-06-23T23:57:47.975009Z"
    }
   },
   "outputs": [],
   "source": [
    "tmp_ge = combined_generator(imgen_train, X_num_tr, contours_tr, shape_contours_tr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T23:59:08.736521Z",
     "start_time": "2018-06-23T23:59:08.701168Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+++++++++++++++++++\n",
      "(32, 511, 2)\n"
     ]
    }
   ],
   "source": [
    "tmp_X,tmp_y = next(tmp_ge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T23:59:25.082502Z",
     "start_time": "2018-06-23T23:59:25.074314Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 96, 96, 1)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_X[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T23:59:37.255781Z",
     "start_time": "2018-06-23T23:59:37.248329Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 192)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_X[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-23T23:59:50.908239Z",
     "start_time": "2018-06-23T23:59:50.899571Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 200, 2)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_X[2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T00:00:02.828310Z",
     "start_time": "2018-06-24T00:00:02.819745Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 511, 2)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_X[3].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-06-24T00:01:45.924196Z",
     "start_time": "2018-06-24T00:01:45.915558Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 99)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp_y.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "nbTranslate": {
   "displayLangs": [
    "*"
   ],
   "hotkey": "alt-t",
   "langInMainMenu": true,
   "sourceLang": "en",
   "targetLang": "fr",
   "useGoogleTranslate": true
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
